{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG\n",
    "\n",
    "Para este ejemplo se mejorará el ejemplo que se creó antes ([aquí](../01-rag/rag.ipynb)), se utilizará la misma fuente de información, pero ahora siguiendo el correcto paso a paso de cómo se debería hacer en realidad (ver [aquí](../rag-explanation.md)). Para este caso, utilizaremos la libreria `langchain` con Claude como LLM, y bases de datos de vectores para almacenar la información de los documentos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importar módulos y todo lo necesario\n",
    "\n",
    "Para el modelo de embedding se puede cambiar a otro en caso de ser necesario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "\n",
    "# Para el indexing\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_nomic import NomicEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "# Para la generación\n",
    "from langchain import hub\n",
    "\n",
    "# Variables de entorno\n",
    "from os import getenv\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "CLAUDE_API_KEY = getenv(\"CLAUDE_API_KEY\")\n",
    "MODEL_NAME = getenv(\"CLAUDE_MODEL_NAME\")\n",
    "NOMIC_API_KEY = getenv(\"NOMIC_API_KEY\")\n",
    "LANGCHAIN_API_KEY = getenv(\"LANGCHAIN_API_KEY\")\n",
    "\n",
    "CHUNK_SIZE = int(getenv(\"CHUNK_SIZE\"))\n",
    "CHUNK_OVERLAP = int(getenv(\"CHUNK_OVERLAP\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm: ChatAnthropic = ChatAnthropic(\n",
    "    api_key=CLAUDE_API_KEY,\n",
    "    model=MODEL_NAME,\n",
    "    temperature=0,\n",
    "    max_tokens=2000,\n",
    ")\n",
    "\n",
    "text_splitter: RecursiveCharacterTextSplitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP\n",
    ")\n",
    "\n",
    "embeddings: NomicEmbeddings = NomicEmbeddings(\n",
    "    model=\"nomic-embed-text-v1.5\",\n",
    "    nomic_api_key=NOMIC_API_KEY\n",
    ")\n",
    "\n",
    "vector_store: Chroma = Chroma(embedding_function=embeddings)\n",
    "\n",
    "prompt = hub.pull(\"rlm/rag-prompt\", api_key=LANGCHAIN_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader: TextLoader = TextLoader(\"./info.md\", \"utf-8\")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': './info.md'}, page_content='# NEBULA\\n\\n## Roadmap\\n\\n### Purpose and Scope of the SAD\\n\\nEl SAD tiene como objetivo documentar la arquitectura de Nebula, detallando los componentes estructurales y sus interacciones para cumplir con los requisitos de desarrollo, escalabilidad y seguridad. En este documento se incluye toda la informaci�n sobre la arquitectura de software de Nebula, destacando aquellos aspectos que influyen en el rendimiento y la adaptabilidad del sistema. Las decisiones de dise�o documentadas en el SAD son aquellas que tienen un impacto arquitect�nico relevante en la estructura general del sistema, mientras que los detalles de implementaci�n espec�ficos se documentan en otros informes t�cnicos.\\n\\n### How the SAD Is Organized'),\n",
       " Document(metadata={'source': './info.md'}, page_content='El SAD de Nebula est� estructurado en las siguientes secciones:  \\nSección 1: Documentation Roadmap - Proporciona una visi�n general del SAD, el prop�sito del documento y una gu�a sobre c�mo est� organizado.  \\nSección 2: Antecedentes de la Arquitectura - Explica los objetivos y el contexto de desarrollo de Nebula, los requerimientos que impulsan las decisiones arquitect�nicas y los enfoques de soluci�n adoptados, as� como un resumen de los cambios entre versiones.  \\nSección 3: Vistas de Arquitectura - Presenta diversas vistas de la arquitectura del software (m�dulos, componentes y conectores, y asignaci�n) que detallan la organizaci�n, relaci�n y comportamiento de los elementos en el sistema.  \\nSección 4: Relaciones entre Vistas - Describe c�mo se relacionan e integran las diferentes vistas arquitect�nicas del sistema, proporcionando una visi�n coherente de la arquitectura de Nebula.'),\n",
       " Document(metadata={'source': './info.md'}, page_content='Sección 4: Relaciones entre Vistas - Describe c�mo se relacionan e integran las diferentes vistas arquitect�nicas del sistema, proporcionando una visi�n coherente de la arquitectura de Nebula.  \\nSección 5: Materiales Referenciados - Cita los documentos de referencia y bibliograf�a utilizados en el SAD, proporcionando una base documental para futuras consultas.  \\nSección 6: Directorio - Incluye un �ndice de t�rminos, glosario y lista de acr�nimos, facilitando la navegaci�n y comprensi�n de t�rminos clave empleados en el documento.')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_splits: list[Document] = text_splitter.split_documents(docs)\n",
    "all_splits[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "store = vector_store.add_documents(all_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a276c71b-9590-4411-b1c2-fcdf16d105e1',\n",
       " 'fc2f7bb7-fcc9-489c-84cc-11d7dfb6f65c',\n",
       " '8b08e589-1098-4cf4-8167-ba645139765e']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recuperación y Generación\n",
    "\n",
    "Con `\"rlm/rag-prompt\"`, se está haciendo este prompt al modelo:\n",
    "\n",
    "```\n",
    "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
    "Question: {question} \n",
    "Context: {context} \n",
    "Answer:\n",
    "```\n",
    "\n",
    "Ver [esto](https://smith.langchain.com/hub/rlm/rag-prompt) para más info\n",
    "\n",
    "\n",
    "En este caso, para la generación de la respuesta, no se utilizará LangGraph, pero también es posible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(question: str):\n",
    "    retrieved_docs = vector_store.similarity_search(question)\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)\n",
    "    message_prompt = {\"question\": question, \"context\": docs_content}\n",
    "    messages = prompt.invoke(message_prompt)\n",
    "    response = llm.invoke(messages)\n",
    "    return {**message_prompt, \"answer\": response.content}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
